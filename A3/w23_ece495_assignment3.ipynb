{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arj1211/ece495/blob/main/A3/w23_ece495_assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQUuhLK1QbsT"
      },
      "source": [
        "# ECE 493 Assignment 3\n",
        "\n",
        "Assignment Overview\n",
        "- Learn pytorch and helper libraries\n",
        "- Understand VOC Sementic Segmentation Dataset\n",
        "- Create data transforms to augment the dataset\n",
        "- Create the neural network\n",
        "- Train and evaluate the neural network\n",
        "\n",
        "Problems:\n",
        "- 1) Data augmentation\n",
        "- 2) Create the neural network\n",
        "\n",
        "Note:\n",
        "- This assignment was created based on assignment 4 of CS 484\n",
        " - https://cs.uwaterloo.ca/~yboykov/Courses/cs484/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUvt-a4zQbse"
      },
      "source": [
        "# Importing libraries\n",
        "\n",
        "Everything that you need for this assignment is imported here.\n",
        "\n",
        "The VoxSegmentation dataset contains images and segmented images.\n",
        "https://chainercv.readthedocs.io/en/stable/reference/datasets.html#chainercv.datasets.VOCSemanticSegmentationDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HZsSlyiQbsk"
      },
      "outputs": [],
      "source": [
        "# You can run this cell to install packages\n",
        "# I believe this is the only package not preinstalled on google colab\n",
        "!pip install chainercv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4swWaUWQbs6"
      },
      "outputs": [],
      "source": [
        "# pytorch library\n",
        "import torch\n",
        "# neural network layers (Conv2d, Linear, etc.) that will be trained\n",
        "# https://pytorch.org/docs/stable/nn.html\n",
        "import torch.nn as nn\n",
        "# Many functions (convolution, pooling, activation, etc.)\n",
        "# https://pytorch.org/docs/stable/nn.functional.html\n",
        "import torch.nn.functional as F\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Contains pretrained models for transfer learning\n",
        "# https://pytorch.org/docs/stable/torchvision/models.html\n",
        "import torchvision.models as models\n",
        "# import data transforms\n",
        "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "# Take note of transforms.ToPILImage and transforms.ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Use cuda on the gpu or use the cpu\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# Overide device when testing on CPU\n",
        "# device = 'cpu'\n",
        "\n",
        "# graphing and images\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# numpy\n",
        "import numpy as np\n",
        "\n",
        "# deepcopy the nn\n",
        "import copy\n",
        "\n",
        "# for random number generation\n",
        "import random\n",
        "\n",
        "# chainerCV is similar to torchvision\n",
        "import chainercv\n",
        "# https://chainercv.readthedocs.io/en/stable/reference/datasets.html#chainercv.datasets.VOCSemanticSegmentationDataset\n",
        "from chainercv.datasets import VOCSemanticSegmentationDataset\n",
        "# https://docs.chainer.org/en/stable/reference/generated/chainer.datasets.TransformDataset.html#chainer.datasets.TransformDataset\n",
        "from chainer.datasets import TransformDataset\n",
        "\n",
        "# Used to calculate mIoU\n",
        "from chainercv.evaluations import eval_semantic_segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKsdN-NxQbtN"
      },
      "source": [
        "# Import VOC Segmentation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYF6D_JqQbtT",
        "outputId": "369f7d37-ed7f-4e25-9b89-d0798c937472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Number of training examples: 1464\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "DATASET_LOCATION = '/content/gdrive/MyDrive/'\n",
        "\n",
        "if not path.exists(DATASET_LOCATION + 'VOC2012'):\n",
        "  print('First time run, downloading VOC and copying to permanent location')\n",
        "  # auto will download the dataset to $HOME/.chainer/dataset\n",
        "  # This gets deleted when you disconnect from google colab\n",
        "  voc_train_data = VOCSemanticSegmentationDataset(data_dir='auto', split='train')\n",
        "  !cp -r $HOME/.chainer/dataset/pfnet/chainercv/voc/VOCdevkit/VOC2012/ /content/gdrive/MyDrive/\n",
        "\n",
        "# Instantiate the data loader with the dataset directory\n",
        "voc_train_data = VOCSemanticSegmentationDataset(data_dir=DATASET_LOCATION + 'VOC2012', split='train')\n",
        "\n",
        "print(\"Number of training examples:\", len(voc_train_data))\n",
        "print(\"Each with one image and one ground truth (gt) segmented image:\", len(voc_train_data[0]))\n",
        "\n",
        "print(\"Image Shape:\", voc_train_data[0][0].shape)\n",
        "print(\"Segmented Image Shape:\", voc_train_data[0][1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rolo1iXUQbtk"
      },
      "source": [
        "There are 22 Classes in VOC.\n",
        "\n",
        "https://github.com/NVIDIA/DIGITS/blob/master/examples/semantic-segmentation/pascal-voc-classes.txt\n",
        "- -1 is ignore\n",
        "- 0 is background\n",
        "- 1-20 are different objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70tpTomUQbtn"
      },
      "outputs": [],
      "source": [
        "# Find images with people and cars\n",
        "def find_car_and_ppl_images():\n",
        "    car_label = 7\n",
        "    person_label = 15\n",
        "    count = 0\n",
        "    max_count = 100\n",
        "    for i in range(len(voc_train_data)):\n",
        "        tmp_count = np.unique(voc_train_data[i][1], return_counts=True)\n",
        "        if car_label in tmp_count[0] and person_label in tmp_count[0]:\n",
        "            if count == max_count:\n",
        "                break\n",
        "            else:\n",
        "                img, label = voc_train_data[i]\n",
        "                fig = plt.figure(figsize=(4,3))\n",
        "                ax = fig.add_subplot(1,1,1)\n",
        "                plt.title('Image ' + str(i))\n",
        "                ax.imshow(np.rollaxis(img.astype(int), 0, 3))\n",
        "                count += 1\n",
        "\n",
        "# Some relevant image indexes for a self-driving car\n",
        "# Car = 119, 273, Bicycle = 225\n",
        "img_119, seg_img_119 = voc_train_data[119]\n",
        "img_273, seg_img_273 = voc_train_data[273]\n",
        "img_225, seg_img_225 = voc_train_data[225]\n",
        "\n",
        "print(\"View count of pixels within each label category:\")\n",
        "print(np.unique(voc_train_data[119][1], return_counts=True))\n",
        "print(np.unique(voc_train_data[273][1], return_counts=True))\n",
        "print(np.unique(voc_train_data[225][1], return_counts=True))\n",
        "\n",
        "print(\"We must move the RGB channel to the back to be viewed with plt:\")\n",
        "print(\"Before:\", img_119.shape)\n",
        "print(\"After:\", np.rollaxis(img_119, 0, 3).shape)\n",
        "\n",
        "print(\"Lastly let's verify that the image and gt label match\")\n",
        "\n",
        "# This colorize_mask class takes in a numpy segmentation mask,\n",
        "#  and then converts it to a PIL Image for visualization.\n",
        "#  Since by default the numpy matrix contains integers from\n",
        "#  0,1,...,num_classes, we need to apply some color to this\n",
        "#  so we can visualize easier! Refer to:\n",
        "#  https://pillow.readthedocs.io/en/4.1.x/reference/Image.html#PIL.Image.Image.putpalette\n",
        "palette = [0, 0, 0, 128, 0, 0, 0, 128, 0, 128, 128, 0, 0, 0, 128, 128, 0, 128, 0, 128, 128,\n",
        "           128, 128, 128, 64, 0, 0, 192, 0, 0, 64, 128, 0, 192, 128, 0, 64, 0, 128, 192, 0, 128,\n",
        "           64, 128, 128, 192, 128, 128, 0, 64, 0, 128, 64, 0, 0, 192, 0, 128, 192, 0, 0, 64, 128]\n",
        "\n",
        "def colorize_mask(mask):\n",
        "    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n",
        "    new_mask.putpalette(palette)\n",
        "\n",
        "    return new_mask\n",
        "\n",
        "def add_img_plot(fig, index, img, title, sub_plot_id):\n",
        "    ax = fig.add_subplot(sub_plot_id[0],sub_plot_id[1],sub_plot_id[2])\n",
        "    plt.title(title + str(index))\n",
        "    ax.imshow(img)\n",
        "\n",
        "# Create the figure size\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "# Car image 119\n",
        "add_img_plot(fig, 119, np.rollaxis(img_119.astype(int), 0, 3), 'Image ', [3,2,1])\n",
        "add_img_plot(fig, 119, colorize_mask(seg_img_119), 'Segmented Image ', [3,2,2])\n",
        "# Car image 273\n",
        "add_img_plot(fig, 273, np.rollaxis(img_273.astype(int), 0, 3), 'Image ', [3,2,3])\n",
        "add_img_plot(fig, 273, colorize_mask(seg_img_273), 'Segmented Image ', [3,2,4])\n",
        "# Bicycle image 225\n",
        "add_img_plot(fig, 225, np.rollaxis(img_225.astype(int), 0, 3), 'Image ', [3,2,5])\n",
        "add_img_plot(fig, 225, colorize_mask(seg_img_225), 'Segmented Image ', [3,2,6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7RnZC90Qbtw"
      },
      "source": [
        "# Problem 1 Data Augmentation\n",
        "\n",
        "The transform below is used to create a transformed training dataset. It currently center crops all images.\n",
        "\n",
        "Modify it so that:\n",
        "- There is a 50% chance the images will be flipped horizontally\n",
        "- Instead of a center crop, a random part of the image is cropped\n",
        "\n",
        "Useful functions:\n",
        "- chainercv.transforms.flip\n",
        "- chainercv.transforms.random_crop\n",
        "\n",
        "Note: The output image and segmented images should match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUdN8TLbQbty"
      },
      "outputs": [],
      "source": [
        "# From ImageNet dataset\n",
        "norm = ([0.485, 0.456, 0.406], \n",
        "        [0.229, 0.224, 0.225])\n",
        "\n",
        "def tensor_to_img(tensor):\n",
        "    # Denormalize tensor and convert to uint8 np array for visualization\n",
        "    denormalize = transforms.Normalize(\n",
        "        mean=[-m / s for m, s in zip(norm[0], norm[1])],\n",
        "        std=[1.0 / s for s in norm[1]]\n",
        "    )\n",
        "    np_img = np.moveaxis((denormalize(tensor).numpy() * 255), -3, -1).astype(np.uint8)\n",
        "    \n",
        "    return np_img\n",
        "\n",
        "# Modify this function\n",
        "def voc_train_transform(in_data):\n",
        "    size = (110,110)\n",
        "    img, seg_img = in_data\n",
        "\n",
        "    # Add channel to seg img\n",
        "    seg_img = seg_img[None]\n",
        "    \n",
        "    # # Center crop\n",
        "    # img = chainercv.transforms.center_crop(img, size)\n",
        "    # seg_img = chainercv.transforms.center_crop(seg_img, size)[0]\n",
        "    \n",
        "    # 50% random horiz flip\n",
        "    if np.random.random() < 0.5:\n",
        "        img = chainercv.transforms.flip(img, x_flip=True)\n",
        "        seg_img = chainercv.transforms.flip(seg_img, x_flip=True)\n",
        "    \n",
        "    # Random crop\n",
        "    img, crop_param = chainercv.transforms.random_crop(img, size, return_param=True)\n",
        "    seg_img = seg_img[:, crop_param['y_slice'], crop_param['x_slice']][0]\n",
        "\n",
        "    # Convert input img to tensor and normalize\n",
        "    img = np.rollaxis(img, 0, 3)\n",
        "    img = transforms.ToPILImage()(np.uint8(img))\n",
        "    img = transforms.ToTensor()(img)\n",
        "    img = transforms.Normalize(norm[0],norm[1])(img)\n",
        "\n",
        "    return img, torch.from_numpy(seg_img.copy()).long()\n",
        "\n",
        "img_119, seg_img_119 = voc_train_transform(voc_train_data[119])\n",
        "img_273, seg_img_273 = voc_train_transform(voc_train_data[273])\n",
        "img_225, seg_img_225 = voc_train_transform(voc_train_data[225])\n",
        "\n",
        "# Create the figure size\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "# Car image 119\n",
        "add_img_plot(fig, 119, tensor_to_img(img_119), 'Image ', [3,2,1])\n",
        "add_img_plot(fig, 119, colorize_mask(seg_img_119.numpy()), 'Segmented Image ', [3,2,2])\n",
        "# Car image 273\n",
        "add_img_plot(fig, 273, tensor_to_img(img_273), 'Image ', [3,2,3])\n",
        "add_img_plot(fig, 273, colorize_mask(seg_img_273.numpy()), 'Segmented Image ', [3,2,4])\n",
        "# Bicycle image 225\n",
        "add_img_plot(fig, 225, tensor_to_img(img_225), 'Image ', [3,2,5])\n",
        "add_img_plot(fig, 225, colorize_mask(seg_img_225.numpy()), 'Segmented Image ', [3,2,6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGJiEzd6Qbt6"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMbKZ8wcQbt8"
      },
      "outputs": [],
      "source": [
        "voc_train_data_transformed = TransformDataset(voc_train_data, voc_train_transform)\n",
        "\n",
        "# epoch = one iteration over the entire dataset\n",
        "# batch size = samples per batch within an epoch\n",
        "# shuffle = Set to True if your data is ordered\n",
        "# num_workers = number of subprocess to use when dataloading, set higher if you have many cpu cores\n",
        "if device == 'cpu':\n",
        "    train_loader = DataLoader(voc_train_data_transformed, batch_size=5, shuffle=True, num_workers=10)\n",
        "else:\n",
        "    train_loader = DataLoader(voc_train_data_transformed, batch_size=50, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri4ZRVHLQbuD"
      },
      "source": [
        "# Problem 2 Create the neural network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = (3, 4)\n",
        "t = tuple(i+2 for i in t)\n",
        "t"
      ],
      "metadata": {
        "id": "V7nxmcV4mL2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stX4Hbt8QbuF"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes, criterion=None):\n",
        "        super(Net, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.criterion = criterion\n",
        "        # Hints:\n",
        "        # Useful layers:\n",
        "        # - nn.Conv2d\n",
        "        # - nn.BatchNorm2d\n",
        "        # - nn.Dropout2d\n",
        "\n",
        "        # Import a pre-trained encoder\n",
        "        self.resnet = models.resnet34(pretrained=True)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=640, out_channels=64, kernel_size=(5,5))\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.do1 = nn.Dropout2d(0.45)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=21, kernel_size=(3,3))\n",
        "\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Fill in model to produce lfinal\n",
        "        inp_dims = inp.size(2), inp.size(3)\n",
        "        print('inp', inp.shape)\n",
        "        \n",
        "        # Hints: Useful functions\n",
        "        # - F.relu        (neuron activation)\n",
        "        # - F.interpolate (interpolate to increase tensor H and W)\n",
        "        # - torch.cat     (concatenate tensors to implement a skip connection)\n",
        "\n",
        "        # Layer 1\n",
        "        x0 = self.resnet.conv1(inp)\n",
        "        print('x0', x0.shape)\n",
        "        x0 = self.resnet.bn1(x0)\n",
        "        print('x0', x0.shape)\n",
        "        x0 = self.resnet.relu(x0)\n",
        "        print('x0', x0.shape)\n",
        "        x0 = self.resnet.maxpool(x0)\n",
        "        print('x0', x0.shape)\n",
        "\n",
        "        # Layer 2\n",
        "        x1 = self.resnet.layer1(x0)\n",
        "        print('x1', x1.shape)\n",
        "\n",
        "        # Layer 3\n",
        "        x2 = self.resnet.layer2(x1)\n",
        "        print('x2', x2.shape)\n",
        "        x2_dims = x2.size(2), x2.size(3)\n",
        "        \n",
        "        # Layer 4\n",
        "        x3 = self.resnet.layer3(x2)\n",
        "        print('x3', x3.shape)\n",
        "        \n",
        "        # Layer 5\n",
        "        x4 = self.resnet.layer4(x3)\n",
        "        print('x4', x4.shape)\n",
        "\n",
        "        c_inp_1 = F.interpolate(x4, size=x2_dims)\n",
        "        print('c_inp_1', c_inp_1.shape)\n",
        "\n",
        "        c_out = torch.cat((c_inp_1, x2), 1)\n",
        "        print('c_out', c_out.shape)\n",
        "\n",
        "\n",
        "        y1 = self.conv1(c_out)\n",
        "        print('y1', y1.shape)\n",
        "        y1 = self.bn1(y1)\n",
        "        print('y1', y1.shape)\n",
        "        y1 = F.relu(y1)\n",
        "        print('y1', y1.shape)\n",
        "        y1 = self.do1(y1)\n",
        "        print('y1', y1.shape)\n",
        "\n",
        "        inp_dims = tuple(i+2 for i in inp_dims)\n",
        "        y2 = F.interpolate(y1, size=inp_dims)\n",
        "        print('y2', y2.shape)\n",
        "\n",
        "        y3 = self.conv2(y2)\n",
        "        print('y3', y3.shape)\n",
        "\n",
        "        # if self.training:\n",
        "        #     return self.criterion(lfinal, gts)\n",
        "        # else:\n",
        "        #     return lfinal\n",
        "\n",
        "        # Return the final prediction\n",
        "        lfinal = y3\n",
        "        return lfinal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8neHdXQ6QbuL",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "untrained_net = Net(21).to(device)\n",
        "untrained_net.eval()\n",
        "\n",
        "sample_img, sample_target = voc_train_data_transformed[119]\n",
        "\n",
        "untrained_output = untrained_net.forward(sample_img[None].to(device))\n",
        "if device != 'cpu':\n",
        "    untrained_output = untrained_output.cpu()\n",
        "untrained_nn_seg_img_119 = torch.argmax(untrained_output.cpu(), dim=1).numpy()[0]\n",
        "\n",
        "# Create the figure size\n",
        "fig = plt.figure(figsize=(8,2))\n",
        "# Car image 119\n",
        "add_img_plot(fig, 119, transforms.ToPILImage()(img_119), 'Image ', [1,3,1])\n",
        "add_img_plot(fig, 119, colorize_mask(seg_img_119.numpy()), 'GT Segmented Image ', [1,3,2])\n",
        "add_img_plot(fig, 119, colorize_mask(untrained_nn_seg_img_119), 'NN Segmented Image', [1,3,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8wwwRAHQbuT"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, net, criterion, optimizer, loss_graph, device):\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        img, gt_seg_img = data.to(device), target.to(device)\n",
        "\n",
        "        # This is a forward pass which also returns loss due to training mode\n",
        "        output = net(img)\n",
        "        loss = criterion(output, gt_seg_img)\n",
        "\n",
        "        # Populate this list to graph the loss\n",
        "        loss_graph.append(loss.item())\n",
        "        \n",
        "        # View the loss within the epoch\n",
        "        print(loss.item())\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMIuK_xBQbui"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%matplotlib inline\n",
        "\n",
        "# Deep copy in order to train the network with the same initialized weights\n",
        "trained_net = copy.deepcopy(untrained_net)\n",
        "trained_net = trained_net.to(device)\n",
        "\n",
        "# set loss function for the net\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# You can change the number of EPOCHS\n",
        "EPOCH = 40\n",
        "\n",
        "# switch to train mode (original untrained_net was set to eval mode)\n",
        "trained_net.train()\n",
        "\n",
        "# You may switch the optimizer and tune the hyperparmeters\n",
        "optimizer = torch.optim.SGD(trained_net.parameters(),\n",
        "                            lr=0.001,\n",
        "                            weight_decay=1e-5,\n",
        "                            momentum=0.9,\n",
        "                            nesterov=False)\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "loss_graph = []\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.subplots_adjust(bottom=0.2,right=0.85,top=0.95)\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "for e in range(EPOCH):\n",
        "    loss = train(train_loader, trained_net, criterion, optimizer, loss_graph, device)\n",
        "    ax.clear()\n",
        "    ax.set_xlabel('iterations')\n",
        "    ax.set_ylabel('loss value')\n",
        "    ax.set_title('Training loss curve for trained net')\n",
        "    ax.plot(loss_graph, label='training loss')\n",
        "    ax.legend(loc='upper right')\n",
        "    fig.canvas.draw()\n",
        "    print(\"Epoch: {} Loss: {}\".format(e, loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge2j1h_7Qbup"
      },
      "outputs": [],
      "source": [
        "def validate(val_loader, net):\n",
        "    iou_arr = []\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            img, gt_seg_img = data.to(device), target.to(device)\n",
        "\n",
        "            output = net(img)\n",
        "\n",
        "            # Convert to numpy arrays\n",
        "            if device != 'cpu':\n",
        "                output = output.cpu()\n",
        "                gt_seg_img = gt_seg_img.cpu()\n",
        "            pred = torch.argmax(output, dim=1).numpy()[0]\n",
        "\n",
        "            gt_np = gt_seg_img.numpy()[0]\n",
        "\n",
        "            conf = eval_semantic_segmentation(pred[None], gt_np[None])\n",
        "\n",
        "            iou_arr.append(conf['miou'])\n",
        "    \n",
        "    return sum(iou_arr) / len(iou_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfe_pFtkQbuw"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYnfDz0YQbuz"
      },
      "outputs": [],
      "source": [
        "# a validation loader is created with a transform that doesn't modify the data\n",
        "voc_val_data = VOCSemanticSegmentationDataset(data_dir=DATASET_LOCATION + 'VOC2012', split='val')\n",
        "\n",
        "def val_transform(in_data):\n",
        "    img, seg_img = in_data\n",
        "\n",
        "    img = np.rollaxis(img, 0, 3)\n",
        "    img = transforms.ToPILImage()(np.uint8(img))\n",
        "    img = transforms.ToTensor()(img)\n",
        "    img = transforms.Normalize(mean=norm[0],std=norm[1])(img)\n",
        "\n",
        "    return img, torch.from_numpy(seg_img.copy()).long()\n",
        "\n",
        "voc_val_data_transformed = TransformDataset(voc_val_data, val_transform)\n",
        "\n",
        "# epoch = one iteration over the entire dataset\n",
        "# batch size = samples per batch within an epoch\n",
        "# shuffle = Set to True if your data is ordered\n",
        "# num_workers = number of subprocess to use when dataloading, set higher if you have many cpu cores\n",
        "val_loader = DataLoader(voc_val_data_transformed, batch_size=1, shuffle=True, num_workers=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_DFOLY7Qbu6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "print(\"mIoU over the training dataset:{}\".format(validate(train_loader, trained_net)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDgSi3DCQbvA"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "print(\"mIoU over the validation dataset:{}\".format(validate(val_loader, trained_net)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y60UC00LQbvG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# switch back to evaluation mode\n",
        "trained_net.eval()\n",
        "\n",
        "def add_img_txt_plot(fig, index, img, title, sub_plot_id, txt):\n",
        "    ax = fig.add_subplot(sub_plot_id[0],sub_plot_id[1],sub_plot_id[2])\n",
        "    plt.title(title + str(index))\n",
        "    ax.text(10, 25, 'mIoU = {:_>8.6f}'.format(txt), fontsize=20, color='white')\n",
        "    ax.imshow(img)\n",
        "\n",
        "def get_nn_seg_img(net, data):\n",
        "    img, gt_seg_img = data\n",
        "\n",
        "    nn_seg_output = net.forward(img[None].cuda())\n",
        "\n",
        "    # computing mIOU (quantitative measure of accuracy for network predictions)\n",
        "    if device != 'cpu':\n",
        "        nn_seg_img = torch.argmax(nn_seg_output, dim=1).cpu().numpy()[0]\n",
        "    else:\n",
        "        nn_seg_img = torch.argmax(nn_seg_output, dim=1).numpy()[0]\n",
        "\n",
        "    gts = gt_seg_img.cpu().numpy()\n",
        "\n",
        "    conf = eval_semantic_segmentation(nn_seg_img[None], gts[None])\n",
        "\n",
        "    print(\"View count of pixels within each label category:\")\n",
        "    print(np.unique(gt_seg_img, return_counts=True))\n",
        "    print(np.unique(nn_seg_img, return_counts=True))\n",
        "\n",
        "    return nn_seg_img, conf['miou']\n",
        "\n",
        "# Some relevant image indexes for a self-driving car\n",
        "# Car = 119, 273, Bicycle = 225\n",
        "img_119, seg_img_119 = voc_train_data[119]\n",
        "img_273, seg_img_273 = voc_train_data[273]\n",
        "img_225, seg_img_225 = voc_train_data[225]\n",
        "\n",
        "# Get NN Output\n",
        "nn_seg_img_119, miou_119 = get_nn_seg_img(trained_net, val_transform(voc_train_data[119]))\n",
        "nn_seg_img_273, miou_273 = get_nn_seg_img(trained_net, val_transform(voc_train_data[273]))\n",
        "nn_seg_img_225, miou_225 = get_nn_seg_img(trained_net, val_transform(voc_train_data[225]))\n",
        "\n",
        "# Create the figure size\n",
        "fig = plt.figure(figsize=(20,15))\n",
        "# Car image 119\n",
        "add_img_plot(fig, 119, np.rollaxis(img_119.astype(int), 0, 3), 'Image ', [3,3,1])\n",
        "add_img_plot(fig, 119, colorize_mask(seg_img_119), 'GT Segmented Image ', [3,3,2])\n",
        "add_img_txt_plot(fig, 119, colorize_mask(nn_seg_img_119), 'NN Segmented Image ', [3,3,3], miou_119)\n",
        "# Car image 273\n",
        "add_img_plot(fig, 273, np.rollaxis(img_273.astype(int), 0, 3), 'Image ', [3,3,4])\n",
        "add_img_plot(fig, 273, colorize_mask(seg_img_273), 'Segmented Image ', [3,3,5])\n",
        "add_img_txt_plot(fig, 273, colorize_mask(nn_seg_img_273), 'NN Segmented Image ', [3,3,6], miou_273)\n",
        "# Bicycle image 225\n",
        "add_img_plot(fig, 225, np.rollaxis(img_225.astype(int), 0, 3), 'Image ', [3,3,7])\n",
        "add_img_plot(fig, 225, colorize_mask(seg_img_225), 'Segmented Image ', [3,3,8])\n",
        "add_img_txt_plot(fig, 225, colorize_mask(nn_seg_img_225), 'NN Segmented Image ', [3,3,9], miou_225)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGTRlmQhQbvM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}